{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm\n",
    "#np.random.seed(2)\n",
    "from keras.applications.vgg19 import VGG19\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Input, BatchNormalization, Activation, MaxPooling2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "sns.set(style='white', context='notebook', palette='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "#train.drop(train.index[:40000], inplace=True)\n",
    "#test.drop(test.index[:27000], inplace=True)\n",
    "\n",
    "Y_train = train[\"label\"]\n",
    "\n",
    "# Drop 'label' column\n",
    "X_train = train.drop(labels = [\"label\"],axis = 1) \n",
    "\n",
    "# free some space\n",
    "#del train \n",
    "\n",
    "#g = sns.countplot(Y_train)\n",
    "#Y_train.value_counts()\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_train = X_train.values.reshape(-1,28,28,1)\n",
    "\n",
    "test = test / 255.0\n",
    "test = test.values.reshape(-1,28,28,1)\n",
    "\n",
    "Y_train = to_categorical(Y_train, num_classes = 10)\n",
    "\n",
    "imsize=X_train.shape[1]\n",
    "datasize=X_train.shape[0]\n",
    "testsize=test.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "zoom_facor=2\n",
    "X_trainZoom=np.zeros((datasize,imsize*zoom_facor,imsize*zoom_facor,1))\n",
    "X_trainRGB=np.zeros((datasize,imsize*zoom_facor,imsize*zoom_facor,3))\n",
    "\n",
    "# Zoom images\n",
    "for image_id in tqdm(range(datasize)):\n",
    "    X_trainZoom[image_id,:,:,0]=scipy.ndimage.zoom(X_train[image_id,:,:,0], zoom_facor, order=1)\n",
    "\n",
    "# Convert data to RGB\n",
    "new_img = np.zeros((imsize*zoom_facor,imsize*zoom_facor,3))\n",
    "for image_id in tqdm(range(datasize)):\n",
    "    new_img[:,:,:] = X_trainZoom[image_id,:,:]\n",
    "    X_trainRGB[image_id,:,:,:]=new_img\n",
    "\n",
    "testZoom=np.zeros((testsize,imsize*zoom_facor,imsize*zoom_facor,1))\n",
    "testRGB=np.zeros((testsize,imsize*zoom_facor,imsize*zoom_facor,3))\n",
    "for image_id in tqdm(range(testsize)):\n",
    "    testZoom[image_id,:,:,0]=scipy.ndimage.zoom(test[image_id,:,:,0], zoom_facor, order=1)\n",
    "\n",
    "for image_id in tqdm(range(testsize)):\n",
    "    new_img[:,:,:] = testZoom[image_id,:,:]\n",
    "    testRGB[image_id,:,:,:]=new_img\n",
    "\n",
    "print(test.shape)\n",
    "print(testZoom.shape)\n",
    "print(testRGB.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data:\n",
    "\n",
    "#img=X_trainRGB[13,:,:,:]\n",
    "#img=testRGB[13,:,:,:]\n",
    "#print(img.shape)\n",
    "#g = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data:\n",
    "\n",
    "random_seed = 2\n",
    "X_trainRGB, X_valRGB, Y_train, Y_val = train_test_split(X_trainRGB, Y_train, test_size = 0.2, random_state=random_seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and traing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "datagen.fit(X_trainRGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base pre-trained model\n",
    "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(56, 56, 3))\n",
    "\n",
    "# Add a new top layer\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# This is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# First: train only the top layers (which were randomly initialized)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40 # Turn epochs to 30 to get 0.9967 accuracy\n",
    "batch_size = 86\n",
    "\n",
    "# Set a learning rate annealer\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=2, factor=0.5, min_lr=0.00001)\n",
    "\n",
    "history = model.fit_generator(datagen.flow(X_trainRGB,Y_train, batch_size=batch_size),\n",
    "                              epochs = epochs, validation_data = (X_valRGB,Y_val),\n",
    "                              verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size\n",
    "                              , callbacks=[learning_rate_reduction])\n",
    "\n",
    "#history = model.fit(X_trainRGB, Y_train, batch_size = batch_size, epochs = epochs, \n",
    "#                    validation_data = (X_valRGb, Y_val), verbose = 1)\n",
    "\n",
    "# Save model and hostory:\n",
    "basename=\"VGG19_\"+str(epochs)+\"epochs\"\n",
    "historydata = pd.DataFrame.from_dict(history.history)\n",
    "historydata.to_csv(basename+\"-history.csv\")\n",
    "model.save(basename+\"-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and accuracy curves for training and validation \n",
    "fig, ax = plt.subplots(2,1)\n",
    "ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "ax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n",
    "legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "ax[1].plot(history.history['acc'], color='b', label=\"Training accuracy\")\n",
    "ax[1].plot(history.history['val_acc'], color='r',label=\"Validation accuracy\")\n",
    "legend = ax[1].legend(loc='best', shadow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict results\n",
    "results = model.predict(testRGB)\n",
    "\n",
    "# select the indix with the maximum probability\n",
    "results = np.argmax(results,axis = 1)\n",
    "\n",
    "results = pd.Series(results,name=\"Label\")\n",
    "\n",
    "submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n",
    "submission.to_csv(basename+\"-submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next few of cells are for analyzing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Predict the values from the validation dataset\n",
    "Y_pred = model.predict(X_valRGB)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(Y_val,axis = 1) \n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
