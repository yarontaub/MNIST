{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "#from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Input, BatchNormalization, Activation, MaxPooling2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "sns.set(style='white', context='notebook', palette='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Drop 'label' column\n",
    "X_train = train.drop(labels = [\"label\"],axis = 1) \n",
    "\n",
    "del train \n",
    "\n",
    "X_train = X_train / 255.0\n",
    "test = test / 255.0\n",
    "X_train = X_train.values.reshape(-1,28,28,1)\n",
    "test = test.values.reshape(-1,28,28,1)\n",
    "\n",
    "zoom_facor=2\n",
    "imsize=X_train.shape[1]\n",
    "\n",
    "X_trainZoom=np.zeros((X_train.shape[0],imsize*zoom_facor,imsize*zoom_facor,1))\n",
    "for image_id in (range(X_train.shape[0])):\n",
    "    X_trainZoom[image_id,:,:,0]=scipy.ndimage.zoom(X_train[image_id,:,:,0], zoom_facor, order=1)\n",
    "\n",
    "testZoom=np.zeros((test.shape[0],imsize*zoom_facor,imsize*zoom_facor,1))\n",
    "for image_id in (range(test.shape[0])):\n",
    "    testZoom[image_id,:,:,0]=scipy.ndimage.zoom(test[image_id,:,:,0], zoom_facor, order=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'new_lables' splits the '4' digit to two kinds of 4's (adding a category)\n",
    "Y_train = pd.read_csv(\"new_lables.csv\")\n",
    "Y_train=Y_train[\"Lable\"]\n",
    "\n",
    "#print(type(Y_train))\n",
    "#print(Y_train.shape)\n",
    "#Y_train.head(50)\n",
    "#print(Y_train.value_counts())\n",
    "\n",
    "Y_train = to_categorical(Y_train, num_classes = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 2\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_trainZoom, Y_train, test_size = 0.2, random_state=random_seed)\n",
    "del X_trainZoom\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data:\n",
    "\n",
    "#g = plt.imshow(X_train[10][:,:,0])\n",
    "#print(Y_train[0:50,0:44])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and traing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.15, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input=Input(shape=(56, 56, 1), name='Input_Layer')\n",
    "\n",
    "X = Conv2D(32, (5, 5), strides = (1, 1), padding='same', name = 'conv1')(X_input)\n",
    "#X = BatchNormalization(axis = 1, epsilon=0.00001, name = 'b1')(X)\n",
    "X = Dropout(0.25, name='drop1')(X)\n",
    "X = Activation('relu', name='activate1')(X)\n",
    "X = MaxPooling2D(pool_size=2, strides=2, name='maxpool1')(X)\n",
    "\n",
    "X = Conv2D(64, (3, 3), strides = (1, 1), padding='same', name = 'conv2')(X)\n",
    "#X = BatchNormalization(axis = 1, epsilon=0.00001, name = 'b2')(X)\n",
    "X = Dropout(0.25, name='drop2')(X)\n",
    "X = Activation('relu', name='activate2')(X)\n",
    "X = MaxPooling2D(pool_size=2, strides=2, name='maxpool2')(X)\n",
    "\n",
    "X = Conv2D(128, (3, 3), strides = (1, 1), padding='same', name = 'conv3')(X)\n",
    "#X = BatchNormalization(axis = 1, epsilon=0.00001, name = 'b3')(X)\n",
    "X = Dropout(0.25, name='drop3')(X)\n",
    "X = Activation('relu', name='activate3')(X)\n",
    "X = MaxPooling2D(pool_size=2, strides=2, name='maxpool3')(X)\n",
    "\n",
    "X = Conv2D(256, (3, 3), strides = (1, 1), padding='same', name = 'conv4')(X)\n",
    "#X = BatchNormalization(axis = 1, epsilon=0.00001, name = 'b4')(X)\n",
    "X = Dropout(0.25, name='drop4')(X)\n",
    "X = Activation('relu', name='activate4')(X)\n",
    "X = MaxPooling2D(pool_size=2, strides=2, name='maxpool4')(X)\n",
    "\n",
    "X = Conv2D(512, (3, 3), strides = (1, 1), padding='same', name = 'conv5')(X)\n",
    "#X = BatchNormalization(axis = 1, epsilon=0.00001, name = 'b5')(X)\n",
    "X = Dropout(0.25, name='drop5')(X)\n",
    "X = Activation('relu', name='activate5')(X)\n",
    "X = MaxPooling2D(pool_size=3, strides=3, name='maxpool5')(X)\n",
    "\n",
    "X = Flatten(name='flatten_to_dense')(X)\n",
    "#X = Dense(512,activation='relu',name='dense1')(X)\n",
    "#X = Dropout(0.5, name='drop_dense1')(X)\n",
    "X = Dense(256,activation='relu',name='dense2')(X)\n",
    "X = Dropout(0.5, name='drop_dense2')(X)\n",
    "X = Dense(11,activation='softmax',name='output')(X)\n",
    "\n",
    "model = Model(inputs=X_input, outputs=X)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = 'Adam' , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Set a learning rate annealer\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=2, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "#callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "390/390 [==============================] - 25s 65ms/step - loss: 0.3326 - acc: 0.9002 - val_loss: 0.2062 - val_acc: 0.9801\n",
      "Epoch 2/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.1288 - acc: 0.9629 - val_loss: 0.1569 - val_acc: 0.9850\n",
      "Epoch 3/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0955 - acc: 0.9725 - val_loss: 0.0993 - val_acc: 0.9881\n",
      "Epoch 4/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0822 - acc: 0.9766 - val_loss: 0.1024 - val_acc: 0.9867\n",
      "Epoch 5/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0747 - acc: 0.9795 - val_loss: 0.0897 - val_acc: 0.9886\n",
      "Epoch 6/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0683 - acc: 0.9804 - val_loss: 0.0454 - val_acc: 0.9904\n",
      "Epoch 7/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0603 - acc: 0.9827 - val_loss: 0.0447 - val_acc: 0.9919\n",
      "Epoch 8/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0629 - acc: 0.9829 - val_loss: 0.0416 - val_acc: 0.9924\n",
      "Epoch 9/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0560 - acc: 0.9846 - val_loss: 0.0602 - val_acc: 0.9898\n",
      "Epoch 10/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0552 - acc: 0.9847 - val_loss: 0.0638 - val_acc: 0.9910\n",
      "Epoch 11/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0519 - acc: 0.9853 - val_loss: 0.0560 - val_acc: 0.9889\n",
      "Epoch 12/100\n",
      "389/390 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9850\n",
      "Epoch 00012: reducing learning rate to 0.000500000023749.\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0537 - acc: 0.9850 - val_loss: 0.0562 - val_acc: 0.9890\n",
      "Epoch 13/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0367 - acc: 0.9885 - val_loss: 0.0285 - val_acc: 0.9939\n",
      "Epoch 14/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0336 - acc: 0.9903 - val_loss: 0.0371 - val_acc: 0.9932\n",
      "Epoch 15/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0298 - acc: 0.9911 - val_loss: 0.0314 - val_acc: 0.9925\n",
      "Epoch 16/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0319 - acc: 0.9909 - val_loss: 0.0388 - val_acc: 0.9937\n",
      "Epoch 17/100\n",
      "389/390 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9908\n",
      "Epoch 00017: reducing learning rate to 0.000250000011874.\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0304 - acc: 0.9908 - val_loss: 0.0316 - val_acc: 0.9930\n",
      "Epoch 18/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0263 - acc: 0.9919 - val_loss: 0.0291 - val_acc: 0.9948\n",
      "Epoch 19/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0234 - acc: 0.9933 - val_loss: 0.0292 - val_acc: 0.9948\n",
      "Epoch 20/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0218 - acc: 0.9943 - val_loss: 0.0280 - val_acc: 0.9939\n",
      "Epoch 21/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0218 - acc: 0.9938 - val_loss: 0.0309 - val_acc: 0.9942\n",
      "Epoch 22/100\n",
      "389/390 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9939\n",
      "Epoch 00022: reducing learning rate to 0.000125000005937.\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0218 - acc: 0.9939 - val_loss: 0.0283 - val_acc: 0.9938\n",
      "Epoch 23/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0175 - acc: 0.9951 - val_loss: 0.0249 - val_acc: 0.9936\n",
      "Epoch 24/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0187 - acc: 0.9943 - val_loss: 0.0250 - val_acc: 0.9935\n",
      "Epoch 25/100\n",
      "389/390 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9951\n",
      "Epoch 00025: reducing learning rate to 6.25000029686e-05.\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0176 - acc: 0.9951 - val_loss: 0.0260 - val_acc: 0.9937\n",
      "Epoch 26/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0169 - acc: 0.9949 - val_loss: 0.0274 - val_acc: 0.9940\n",
      "Epoch 27/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0154 - acc: 0.9953 - val_loss: 0.0256 - val_acc: 0.9943\n",
      "Epoch 28/100\n",
      "389/390 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9952\n",
      "Epoch 00028: reducing learning rate to 3.12500014843e-05.\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0172 - acc: 0.9953 - val_loss: 0.0278 - val_acc: 0.9942\n",
      "Epoch 29/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0135 - acc: 0.9963 - val_loss: 0.0250 - val_acc: 0.9945\n",
      "Epoch 30/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0144 - acc: 0.9955 - val_loss: 0.0242 - val_acc: 0.9940\n",
      "Epoch 31/100\n",
      "389/390 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9956\n",
      "Epoch 00031: reducing learning rate to 1.56250007421e-05.\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0132 - acc: 0.9956 - val_loss: 0.0234 - val_acc: 0.9942\n",
      "Epoch 32/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0141 - acc: 0.9960 - val_loss: 0.0234 - val_acc: 0.9940\n",
      "Epoch 33/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0149 - acc: 0.9959 - val_loss: 0.0237 - val_acc: 0.9945\n",
      "Epoch 34/100\n",
      "389/390 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9957\n",
      "Epoch 00034: reducing learning rate to 1e-05.\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0154 - acc: 0.9957 - val_loss: 0.0236 - val_acc: 0.9944\n",
      "Epoch 35/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0138 - acc: 0.9958 - val_loss: 0.0237 - val_acc: 0.9944\n",
      "Epoch 36/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0141 - acc: 0.9958 - val_loss: 0.0236 - val_acc: 0.9945\n",
      "Epoch 37/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0137 - acc: 0.9956 - val_loss: 0.0236 - val_acc: 0.9943\n",
      "Epoch 38/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0137 - acc: 0.9958 - val_loss: 0.0233 - val_acc: 0.9944\n",
      "Epoch 39/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0118 - acc: 0.9963 - val_loss: 0.0235 - val_acc: 0.9945\n",
      "Epoch 40/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0150 - acc: 0.9955 - val_loss: 0.0236 - val_acc: 0.9945\n",
      "Epoch 41/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0137 - acc: 0.9957 - val_loss: 0.0237 - val_acc: 0.9944\n",
      "Epoch 42/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0134 - acc: 0.9961 - val_loss: 0.0237 - val_acc: 0.9943\n",
      "Epoch 43/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0146 - acc: 0.9953 - val_loss: 0.0235 - val_acc: 0.9945\n",
      "Epoch 44/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0141 - acc: 0.9960 - val_loss: 0.0239 - val_acc: 0.9942\n",
      "Epoch 45/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0139 - acc: 0.9959 - val_loss: 0.0240 - val_acc: 0.9943\n",
      "Epoch 46/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0168 - acc: 0.9959 - val_loss: 0.0244 - val_acc: 0.9946\n",
      "Epoch 47/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0149 - acc: 0.9955 - val_loss: 0.0245 - val_acc: 0.9943\n",
      "Epoch 48/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0123 - acc: 0.9964 - val_loss: 0.0239 - val_acc: 0.9944\n",
      "Epoch 49/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0141 - acc: 0.9956 - val_loss: 0.0240 - val_acc: 0.9942\n",
      "Epoch 50/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0141 - acc: 0.9958 - val_loss: 0.0240 - val_acc: 0.9940\n",
      "Epoch 51/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0130 - acc: 0.9964 - val_loss: 0.0238 - val_acc: 0.9944\n",
      "Epoch 52/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0135 - acc: 0.9959 - val_loss: 0.0235 - val_acc: 0.9945\n",
      "Epoch 53/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0129 - acc: 0.9964 - val_loss: 0.0236 - val_acc: 0.9944\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0146 - acc: 0.9958 - val_loss: 0.0241 - val_acc: 0.9946\n",
      "Epoch 55/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0136 - acc: 0.9961 - val_loss: 0.0237 - val_acc: 0.9946\n",
      "Epoch 56/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0121 - acc: 0.9962 - val_loss: 0.0238 - val_acc: 0.9948\n",
      "Epoch 57/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0135 - acc: 0.9958 - val_loss: 0.0240 - val_acc: 0.9946\n",
      "Epoch 58/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0146 - acc: 0.9957 - val_loss: 0.0240 - val_acc: 0.9945\n",
      "Epoch 59/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0131 - acc: 0.9960 - val_loss: 0.0242 - val_acc: 0.9943\n",
      "Epoch 60/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0135 - acc: 0.9958 - val_loss: 0.0240 - val_acc: 0.9944\n",
      "Epoch 61/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0123 - acc: 0.9965 - val_loss: 0.0235 - val_acc: 0.9942\n",
      "Epoch 62/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0134 - acc: 0.9962 - val_loss: 0.0235 - val_acc: 0.9944\n",
      "Epoch 63/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0119 - acc: 0.9964 - val_loss: 0.0235 - val_acc: 0.9944\n",
      "Epoch 64/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0131 - acc: 0.9960 - val_loss: 0.0232 - val_acc: 0.9948\n",
      "Epoch 65/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0131 - acc: 0.9960 - val_loss: 0.0236 - val_acc: 0.9948\n",
      "Epoch 66/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0127 - acc: 0.9963 - val_loss: 0.0237 - val_acc: 0.9943\n",
      "Epoch 67/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0136 - acc: 0.9962 - val_loss: 0.0239 - val_acc: 0.9943\n",
      "Epoch 68/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0117 - acc: 0.9964 - val_loss: 0.0236 - val_acc: 0.9943\n",
      "Epoch 69/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0128 - acc: 0.9961 - val_loss: 0.0238 - val_acc: 0.9940\n",
      "Epoch 70/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0131 - acc: 0.9961 - val_loss: 0.0235 - val_acc: 0.9942\n",
      "Epoch 71/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0128 - acc: 0.9964 - val_loss: 0.0236 - val_acc: 0.9942\n",
      "Epoch 72/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0113 - acc: 0.9958 - val_loss: 0.0239 - val_acc: 0.9940\n",
      "Epoch 73/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0129 - acc: 0.9964 - val_loss: 0.0241 - val_acc: 0.9942\n",
      "Epoch 74/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0156 - acc: 0.9954 - val_loss: 0.0240 - val_acc: 0.9942\n",
      "Epoch 75/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0118 - acc: 0.9965 - val_loss: 0.0235 - val_acc: 0.9943\n",
      "Epoch 76/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0112 - acc: 0.9966 - val_loss: 0.0235 - val_acc: 0.9943\n",
      "Epoch 77/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0133 - acc: 0.9964 - val_loss: 0.0232 - val_acc: 0.9943\n",
      "Epoch 78/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0125 - acc: 0.9959 - val_loss: 0.0234 - val_acc: 0.9943\n",
      "Epoch 79/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0139 - acc: 0.9961 - val_loss: 0.0239 - val_acc: 0.9943\n",
      "Epoch 80/100\n",
      "390/390 [==============================] - 25s 63ms/step - loss: 0.0132 - acc: 0.9963 - val_loss: 0.0238 - val_acc: 0.9945\n",
      "Epoch 81/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0119 - acc: 0.9965 - val_loss: 0.0236 - val_acc: 0.9942\n",
      "Epoch 82/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0138 - acc: 0.9959 - val_loss: 0.0236 - val_acc: 0.9944\n",
      "Epoch 83/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0128 - acc: 0.9958 - val_loss: 0.0238 - val_acc: 0.9945\n",
      "Epoch 84/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0114 - acc: 0.9968 - val_loss: 0.0234 - val_acc: 0.9943\n",
      "Epoch 85/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0121 - acc: 0.9963 - val_loss: 0.0243 - val_acc: 0.9940\n",
      "Epoch 86/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0109 - acc: 0.9964 - val_loss: 0.0235 - val_acc: 0.9944\n",
      "Epoch 87/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0129 - acc: 0.9964 - val_loss: 0.0236 - val_acc: 0.9946\n",
      "Epoch 88/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0131 - acc: 0.9961 - val_loss: 0.0236 - val_acc: 0.9944\n",
      "Epoch 89/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0116 - acc: 0.9963 - val_loss: 0.0237 - val_acc: 0.9945\n",
      "Epoch 90/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0116 - acc: 0.9962 - val_loss: 0.0235 - val_acc: 0.9945\n",
      "Epoch 91/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0124 - acc: 0.9969 - val_loss: 0.0238 - val_acc: 0.9944\n",
      "Epoch 92/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0117 - acc: 0.9965 - val_loss: 0.0238 - val_acc: 0.9944\n",
      "Epoch 93/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0111 - acc: 0.9967 - val_loss: 0.0231 - val_acc: 0.9946\n",
      "Epoch 94/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0124 - acc: 0.9959 - val_loss: 0.0232 - val_acc: 0.9945\n",
      "Epoch 95/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0118 - acc: 0.9967 - val_loss: 0.0228 - val_acc: 0.9946\n",
      "Epoch 96/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0115 - acc: 0.9964 - val_loss: 0.0226 - val_acc: 0.9943\n",
      "Epoch 97/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0112 - acc: 0.9965 - val_loss: 0.0226 - val_acc: 0.9943\n",
      "Epoch 98/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0127 - acc: 0.9961 - val_loss: 0.0230 - val_acc: 0.9943\n",
      "Epoch 99/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0120 - acc: 0.9963 - val_loss: 0.0233 - val_acc: 0.9943\n",
      "Epoch 100/100\n",
      "390/390 [==============================] - 25s 64ms/step - loss: 0.0107 - acc: 0.9969 - val_loss: 0.0232 - val_acc: 0.9943\n"
     ]
    }
   ],
   "source": [
    "epochs = 100 # Turn epochs to 30 to get 0.9967 accuracy\n",
    "batch_size = 86\n",
    "\n",
    "history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n",
    "                              epochs = epochs, validation_data = (X_val,Y_val),\n",
    "                              verbose = 1, steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "                              callbacks=[learning_rate_reduction])\n",
    "\n",
    "#history = model.fit(X_train, Y_train, batch_size = batch_size, epochs = epochs, \n",
    "#                    validation_data = (X_val, Y_val), verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/matplotlib/artist.py:210: MatplotlibDeprecationWarning: This has been deprecated in mpl 1.5, please use the\n",
      "axes property.  A removal date has not been set.\n",
      "  warnings.warn(_get_axes_msg, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d37ea4810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss and accuracy curves for training and validation \n",
    "fig, ax = plt.subplots(2,1)\n",
    "ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "ax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n",
    "legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "ax[1].plot(history.history['acc'], color='b', label=\"Training accuracy\")\n",
    "ax[1].plot(history.history['val_acc'], color='r',label=\"Validation accuracy\")\n",
    "legend = ax[1].legend(loc='best', shadow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save model and hostory:\n",
    "basename=\"zoom_newlables_5CNN_1Dense_\"+str(epochs)+\"epochs\"\n",
    "historydata = pd.DataFrame.from_dict(history.history)\n",
    "historydata.to_csv(basename+\"-history.csv\")\n",
    "#model.save(basename+\"-model.h5\")\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(basename+\"-model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(basename+\"-wights.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict results\n",
    "\n",
    "# THIS HAS POTENTIAL ERROR - THE '4' CATEGORIES AREN'T MERGED!!!!!!!!!!!!!\n",
    "\n",
    "#results = model.predict(testZoom)\n",
    "\n",
    "# select the indix with the maximum probability\n",
    "#results = np.argmax(results,axis = 1)\n",
    "\n",
    "#results = pd.Series(results,name=\"Label\")\n",
    "#results[results==10]=4\n",
    "\n",
    "submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n",
    "submission.to_csv(basename+\"-submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next few of cells are for analyzing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Predict the values from the validation dataset\n",
    "Y_pred = model.predict(X_val)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(Y_val,axis = 1) \n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_classes[Y_pred_classes==10]=4\n",
    "Y_true[Y_true==10]=4\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "plot_confusion_matrix(confusion_mtx, classes = range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Errors are difference between predicted labels and true labels\n",
    "errors = (Y_pred_classes - Y_true != 0)\n",
    "\n",
    "Y_pred_classes_errors = Y_pred_classes[errors]\n",
    "Y_pred_errors = Y_pred[errors]\n",
    "Y_true_errors = Y_true[errors]\n",
    "X_val_errors = X_val[errors]\n",
    "\n",
    "def display_errors(errors_index,img_errors,pred_errors, obs_errors):\n",
    "    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n",
    "    n = 0\n",
    "    nrows = 2\n",
    "    ncols = 3\n",
    "    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n",
    "    for row in range(nrows):\n",
    "        for col in range(ncols):\n",
    "            error = errors_index[n]\n",
    "            ax[row,col].imshow((img_errors[error]).reshape((56,56)))\n",
    "            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n",
    "            n += 1\n",
    "\n",
    "# Probabilities of the wrong predicted numbers\n",
    "Y_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n",
    "\n",
    "# Predicted probabilities of the true values in the error set\n",
    "true_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n",
    "\n",
    "# Difference between the probability of the predicted label and the true label\n",
    "delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n",
    "\n",
    "# Sorted list of the delta prob errors\n",
    "sorted_dela_errors = np.argsort(delta_pred_true_errors)\n",
    "\n",
    "# Top 6 errors \n",
    "most_important_errors = sorted_dela_errors[-6:]\n",
    "\n",
    "# Show the top 6 errors\n",
    "display_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
